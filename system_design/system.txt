
The Open Systems Interconnection (OSI) 
Summary of Layer Functions:

Physical Layer: Concerned with the physical connection between devices and the transmission of 
raw data bits.
Data Link Layer: Responsible for node-to-node data transfer and error detection/correction.
Controls how devices on the network gain access to the data and permission to transmit it.
Network Layer: Manages the routing of data packets between devices on different networks.
Transport Layer: Ensures reliable data transfer with error recovery and flow control.
Session Layer: Manages sessions or connections between applications.
Presentation Layer: Translates, encrypts, and compresses data for the application layer.
Application Layer: Provides network services and applications to the end-user.

Communication Between Layers:

Each layer only interacts directly with the layers immediately above and below it.
Data is encapsulated with relevant protocol information as it moves down the 
layers and decapsulated as it moves up.

Example of Data Flow:

When you access a website:

Application Layer: Your browser (HTTP request) requests a webpage.
Presentation Layer: The request is formatted and encrypted.
Session Layer: A session is established between your computer and the web server.
Transport Layer: The data is broken into segments (TCP/UDP).
Network Layer: Each segment is encapsulated in a packet with a destination IP address.
Data Link Layer: Each packet is framed with a MAC address for the next-hop network device.
Physical Layer: The frames are transmitted as bits over the physical medium (e.g., Ethernet)


non functional charahteristics

Availability is the percentage of time that some service or infrastructure is accessible 
to clients and is operated upon under normal conditions. For example, if a service has
100% availability, it means that the said service functions and responds as intended
(operates normally) all the time.

availability = Total time - sum of down time // Total time

Reliability, R, is the probability that the service will perform its functions for a
specified time. R measures how the service performs under varying operating conditions.
We often use mean time between failures (MTBF) and mean time to repair (MTTR) as metrics to measure R. 

MTBF = Total time - sum of down time // total number of failures
MTTR = Total maintainance time // total number of repairs

Scalability is the ability of a system to handle an increasing amount of workload
 without compromising performance.

Maintainability, M, is the probability that the service will restore its functions
within a specified time of fault occurrence. M measures how conveniently and swiftly
the service regains its normal operating conditions.
For example, suppose a component has a defined maintainability value of 95% for half an hour.
In that case, the probability of restoring the component to its fully active form in half an hour is 0.95.

MTTR = Total maintainance time // total number of repairs

Fault tolerance refers to a system’s ability to execute persistently even if one or more of its components fail.

Replication
One of the most widely-used techniques is replication-based fault tolerance.
With this technique, we can replicate both the services and data. 
We can swap out failed nodes with healthy ones and a failed data store with its replica. 

Checkpointing
Checkpointing is a technique that saves the system’s state in stable storage for later 
retrieval in case of failures due to errors or service disruptions.

A state is consistent in which all the individual processes of a system have a consistent
view of the shared state or sequence of events that have occurred in a system.

Back-of-the-envelope Numbers
The number of concurrent TCP connections a server can support

The number of requests per second (RPS) a web, database, or cache server can handle

The storage requirements of a service

IO bandwidth calculations

Server types
Web servers
Web servers are the first point of contact after load balancers. 
Depending on the service that’s offered, the memory and storage 
resources in web servers can be small to medium. However, such servers 
require good processing resources.

Application servers
Application servers run the core application software and business logic.
They can require extensive computational and storage resources. 

Storage servers
Blob storage: This is used for its encoded videos.

Temporary processing queue storage: This can hold a few 
hundred hours of video content uploaded daily to YouTube for processing.

Bigtable: This is a specialized storage used for storing a large number of thumbnails of videos.

Relational database management system (RDBMS): This is for users’ and videos’ 
metadata (comments, likes, user channels, and so on).

Component
	
Latencies

L1 cache reference 0.9(nanoseconds)

L2 cache reference 2.8(nanoseconds)

L3 cache reference 12.9(nanoseconds)

Main memory reference 100(nanoseconds)

Read 1 MB sequentially from memory (9 microseconds)

Read 1 MB sequentially from SSD (200 microseconds)

Round trip within same datacenter (500 microseconds)

Read 1 MB sequentially from SSD with speed ~1GB/sec SSD (1 milliseconds)

Disk seek (4 milliseconds)

Read 1 MB sequentially from disk (2 milliseconds)

Send packet SF->NYC 71 milliseconds
	
Important Rates (QPS)
QPS handled by MySQL 1000

QPS handled by key-value store 10,000

QPS handled by cache server 100000-1M

CPU-bound request takes 1X time units to complete some work on a node, 
memory-bound workloads are an order of magnitude slower (10X), 
IO-bound workloads are two orders of magnitude slower (100X) than the CPU-bound workload.
	
Algorithms of load balancers

Round-robin scheduling: In this algorithm, each request is forwarded to a server 
in the pool in a repeating sequential manner.

Weighted round-robin: If some servers have a higher capability of serving clients’ requests,
then it’s preferred to use a weighted round-robin algorithm. 

Least connections: we can use algorithms like least connections where
newer arriving requests are assigned to servers with fewer existing connections. 
LBs keep a state of the number and mapping of existing connections in such a scenario. 

Least response time: In performance-sensitive services, algorithms such as least 
response time are required. This algorithm ensures that the server with the least
response time is requested to serve the clients.

IP hash: Some applications provide a different level of service to users based on 
their IP addresses. In that case, hashing the IP address is performed to 
assign users’ requests to servers.

URL hash: It may be possible that some services within the application are provided
by specific servers only. In that case, a client requesting service from a URL is 
assigned to a certain cluster or set of servers. The URL hashing algorithm is used 
in those scenarios.

Stateful versus stateless LBs

a state is maintained to hold session information of different clients with hosting servers.
a state maintained across different load balancers is considered as stateful load balancing. 
Whereas, a state maintained within a load balancer for internal use is assumed as stateless load balancing.

Layer 4 Load Balancer:

    Scenario: Distributing incoming TCP connections for a database cluster.
    Decision: Routes traffic based on the destination IP and port, using a round-robin algorithm.

Layer 7 Load Balancer:

    Scenario: Managing HTTP traffic for a web application.
    Decision: Inspects the content of the incoming traffic, 
    including HTTP headers, cookies, URLs, and even the data payload.
    Can make decisions based on more granular information such as the requested URL,
    type of content, or user identity.

ACID in detail:

    Atomicity: A transaction is considered an atomic unit. Therefore, either all the statements 
    within a transaction will successfully execute, or none of them will execute.
    If a statement fails within a transaction, it should be aborted and rolled back.

    Consistency: At any given time, the database should be in a consistent state, and it
    should remain in a consistent state after every transaction. For example, 
    if multiple users want to view a record from the database, it should return a
    similar result each time.

    Isolation: In the case of multiple transactions running concurrently, they shouldn’t be 
    affected by each other. The final state of the database should be the same as the
    transactions were executed sequentially.

    Durability: The system should guarantee that completed transactions will survive permanently 
    in the database even in system failure events.

Quorum as a solution

A quorum in a distributed system is the minimal number of replicas on which a distributed operation 
(commit/abort) must be completed before proclaiming the operation’s success.

Selecting quorum number
We should select more than half of the replicas in the cluster. If there are R replicas in the cluster,
we should choose R/2+1 as a quorum number.

ZooKeeper

To track changes in the cluster, many distributed data systems need a separate management server
like ZooKeeper. Zookeeper keeps track of all the mappings in the network, and each node connects 
to ZooKeeper for the information. Whenever there’s a change in the partitioning, or a node is
added or removed, ZooKeeper gets updated and notifies the routing tier about the change.
HBase, Kafka and SolrCloud use ZooKeeper.

Partitioning secondary indexes by document in a database

This approach involves creating secondary indexes that are partitioned according
to the primary key or document ID, ensuring that the index entries related to a 
particular document are stored together. 

Partitioning secondary indexes by the term in a database
Distributing index entries across partitions based on terms helps balance the load, 
preventing any single partition from becoming a bottleneck.
Queries can be processed in parallel across multiple partitions, improving query performance and reducing latency.
For range queries (e.g., finding all emails starting with a particular letter), the partitioning scheme can be 
designed to keep related terms together, optimizing range query performance.

Data cardinality refers to the uniqueness of data values contained in a column (attribute) of a database.
High Cardinality:

    An attribute with high cardinality contains a large number of unique values.
    Examples: Social Security Numbers, email addresses, or user IDs.
Low Cardinality:

    An attribute with low cardinality contains a small number of unique values.
    Examples: Boolean fields (true/false), gender (male/female), or status (active/inactive).
Unique Cardinality:

    Every value in the column is unique.
    Examples: Primary keys, such as user IDs or order IDs.

Non-Unique Cardinality:

    Values in the column are not unique and may repeat.
    Examples: Country names in a customer table, product categories in an inventory table.

Indexing:

    High cardinality attributes are often indexed to improve query performance, as the index 
    can quickly narrow down search results.

Key-value stores are useful in many situations, such as storing user sessions in a
web application and building NoSQL databases.


Consistent hashing
Consistent hashing is an effective way to manage the load over the set of nodes. 
In consistent hashing, we consider that we have a conceptual ring of hashes from 00 to n−1,
where nn is the number of available hash values. We use each node’s ID, calculate its hash, 
and map it to the ring. We apply the same process to requests. Each request is completed by 
the next node that it finds by moving in the clockwise direction in the ring.

Use virtual nodes
We’ll use virtual nodes to ensure a more evenly distributed load across the nodes. 
Instead of applying a single hash function, we’ll apply multiple hash functions onto the same key.

Data versioning
To handle inconsistency, we need to maintain causality between the events

A vector clock is a mechanism for tracking causality and the partial ordering
of events in distributed systems. It helps in understanding the sequence and 
causality of events, allowing the system to determine whether two events are
causally related, concurrent, or if one event happened before another.